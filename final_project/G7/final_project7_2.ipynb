{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 三 Bert classification ","metadata":{}},{"cell_type":"code","source":"!pip install transformers #安裝 transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:16:25.363598Z","iopub.execute_input":"2022-01-18T12:16:25.363981Z","iopub.status.idle":"2022-01-18T12:16:36.317034Z","shell.execute_reply.started":"2022-01-18T12:16:25.363860Z","shell.execute_reply":"2022-01-18T12:16:36.315987Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Libraries\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport csv\nimport os\nfrom IPython.display import clear_output\n\n# Models\n\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertForMaskedLM\n\n# Training\n\nimport torch.optim as optim\n\n# Evaluation\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:16:36.319838Z","iopub.execute_input":"2022-01-18T12:16:36.320826Z","iopub.status.idle":"2022-01-18T12:16:45.777659Z","shell.execute_reply.started":"2022-01-18T12:16:36.320779Z","shell.execute_reply":"2022-01-18T12:16:45.776704Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 詳細見 https://huggingface.co/transformers/pretrained_models.html\n\nPRETRAINED_MODEL_NAME = \"bert-base-multilingual-uncased\" # 因為我們的逐字稿有中英文\n\n# 取得此預訓練模型所使用的 tokenizer ，事實上這就是別人已經訓練好的Bert model\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n\n# 我們來看看這個pre-trained model 中的內容 \nvocab = tokenizer.vocab\n\nclear_output()\nprint(\"PyTorch 版本：\", torch.__version__)\nprint(len(vocab)) #字典長度 \nprint(list(vocab.items())[0:3]) #字典的token 與 index","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:16:45.779383Z","iopub.execute_input":"2022-01-18T12:16:45.779692Z","iopub.status.idle":"2022-01-18T12:16:52.133963Z","shell.execute_reply.started":"2022-01-18T12:16:45.779636Z","shell.execute_reply":"2022-01-18T12:16:52.132742Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\nrandom_tokens = random.sample(list(vocab), 10)\nrandom_ids = [vocab[t] for t in random_tokens]\n\nprint(\"{0:20}{1:15}\".format(\"token\", \"index\"))\nprint(\"-\" * 25)\nfor t, id in zip(random_tokens, random_ids):\n    print(\"{0:15}{1:10}\".format(t, id))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:16:52.136682Z","iopub.execute_input":"2022-01-18T12:16:52.137012Z","iopub.status.idle":"2022-01-18T12:16:52.159597Z","shell.execute_reply.started":"2022-01-18T12:16:52.136970Z","shell.execute_reply":"2022-01-18T12:16:52.158173Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"因為我們採用的是多語言的 bert 因此可以看到目前的字典中有許多不同的語言","metadata":{}},{"cell_type":"code","source":"# 把護理師的一段逐字稿透過 pre-trained bert 來斷句看看\n\ntext = \"這你知道嗎他可以吃了，對on full diet\"\ntoken = tokenizer.tokenize(text)\nid = tokenizer.convert_tokens_to_ids(token)\n\nprint(text)\nprint(token)\nprint(id)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:17:39.675768Z","iopub.execute_input":"2022-01-18T12:17:39.676074Z","iopub.status.idle":"2022-01-18T12:17:39.685551Z","shell.execute_reply.started":"2022-01-18T12:17:39.676044Z","shell.execute_reply":"2022-01-18T12:17:39.684065Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 除了一般的 word and wordpiece 為token之外，還有以下特殊的token\n\n\"\"\"\n[CLS]：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.\n[SEP]：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔\n[UNK]：沒出現在 BERT 字典裡頭的字會被這個 token 取代\n[PAD]：zero padding 遮罩，將長度不一的輸入序列補齊方便做 batch 運算\n[MASK]：未知遮罩，僅在預訓練階段會用到\n\"\"\"\n\ntext = \"[CLS] 反正就BUN、creatinine都還Ok就抽血這之前的 [MASK] \"\ntoken = tokenizer.tokenize(text)\nid = tokenizer.convert_tokens_to_ids(token)\n\nprint(text)\nprint(token)\nprint(id)\n\n## ##inin 表示目前的辭典中沒有 creatinine 這個單辭，因此分成三個單詞","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:17:41.974756Z","iopub.execute_input":"2022-01-18T12:17:41.975521Z","iopub.status.idle":"2022-01-18T12:17:41.985367Z","shell.execute_reply.started":"2022-01-18T12:17:41.975488Z","shell.execute_reply":"2022-01-18T12:17:41.983945Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokens_tensor = torch.tensor([id]) # [1:10]\nsegments_tensors = torch.zeros_like(tokens_tensor)\nmaskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME) #使用 BertForMaskedLM model \nclear_output()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:17:44.091533Z","iopub.execute_input":"2022-01-18T12:17:44.092281Z","iopub.status.idle":"2022-01-18T12:18:11.275900Z","shell.execute_reply.started":"2022-01-18T12:17:44.092244Z","shell.execute_reply":"2022-01-18T12:18:11.274250Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \nmaskedLM_model.eval()\nwith torch.no_grad():\n    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n    predictions = outputs[0]\n    # (1, seq_len, num_hidden_units)\ndel maskedLM_model\n\n# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\nmasked_index = 5\nk = 3\nprobs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\npredicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n\n# 顯示 top k 可能的字。一般我們就是取 top 1 當作預測值\nprint(\"輸入 tokens ：\", token[:10], '...')\nprint('-' * 50)\nfor i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n    token[masked_index] = t\n    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), token[:10]), '...')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:17.590104Z","iopub.execute_input":"2022-01-18T12:18:17.590695Z","iopub.status.idle":"2022-01-18T12:18:17.991044Z","shell.execute_reply.started":"2022-01-18T12:18:17.590645Z","shell.execute_reply":"2022-01-18T12:18:17.988899Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"metadata = pd.read_csv('../input/medical-record-nlp-for-ner-task/MedData.csv',encoding = 'utf-8')\ndata = metadata[['raw_data','class']]\ntrain = data.sample(frac=0.7, random_state=20220117)\nvalid = data.drop(train.index)\ntrain.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\nvalid.to_csv(\"valid.tsv\", sep=\"\\t\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:20.227528Z","iopub.execute_input":"2022-01-18T12:18:20.227842Z","iopub.status.idle":"2022-01-18T12:18:20.276826Z","shell.execute_reply.started":"2022-01-18T12:18:20.227788Z","shell.execute_reply":"2022-01-18T12:18:20.275919Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 這個步驟很關鍵，我們知道Bert 是 transformers 的 encoder ，表示每一個 input sequence都會跟其他的input sequence做\n# 交互作用 最終得到一個output sequence 而這個過程就稱為 self-attention，然而 input 本身必須先被embedding\n# 而bert 針對 word embedding 可以拆解成 三個 part ，分別為 token embeddings、segment embeddings, 和position embeddings。\n# 詳細說明可以看 以下網址 : https://www.cnblogs.com/d0main/p/10447853.html\n\n\"\"\"\ntokens_tensor：代表識別每個 token 的索引值，用 tokenizer 轉換即可 (Bert 中一個token embedding 為768維)\nsegments_tensor：用來識別句子界限。第一句為 0，第二句則為 1 等等。另外注意句子間的 [SEP] 為 0 \n ##### 註解 :因為有的時候我們讀進去的句子不一定是一句話，而是一段話，因此我們需要藉由該參數協助machine分辨有幾句話\nmasks_tensor：用來界定自注意力機制範圍。1 讓 BERT 關注該位置，0 則代表是 padding 不需關注\n ##### 註解 :其實上面的說法不是很精確，之所以需要position embedding，主要是因為self-attention\n             最大的問題在於為了實踐平行計算而丟失序列本身有前後關係的特性，因此為了使得序列之間的關係得以\n             在使用self attention的時候得以保留，我們加入位置參數，告訴machine 每個token 的相對位置。\n\"\"\" \n\nclass Med_Data(torch.utils.data.Dataset):\n    def __init__(self, mode, tokenizer):\n        \n        self.mode = mode # 'train', 'val' or 'test'\n        self.df = pd.read_csv(\"./\"+mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n        self.len = len(self.df)\n        self.tokenizer = tokenizer\n\n    def __getitem__(self, index):\n        if self.mode == \"test\":\n            text = self.df.iloc[index,1]\n            label_tensor = None\n        else:\n            text, label = self.df.iloc[index, :].values\n            label_tensor = torch.tensor(label)\n\n      #  set up BERT tokens\n        word_pieces = [\"[CLS]\"]\n        tokens = self.tokenizer.tokenize(text)\n        word_pieces += tokens + [\"[SEP]\"]\n\n      # 將整個 token 序列轉換成索引序列\n        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n        tokens_tensor = torch.tensor(ids)\n\n        return (tokens_tensor, label_tensor)\n    \n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:22.236880Z","iopub.execute_input":"2022-01-18T12:18:22.237593Z","iopub.status.idle":"2022-01-18T12:18:22.248204Z","shell.execute_reply.started":"2022-01-18T12:18:22.237553Z","shell.execute_reply":"2022-01-18T12:18:22.246985Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset_train = Med_Data(\"train\", tokenizer=tokenizer)\ndataset_val = Med_Data(\"valid\", tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:24.707660Z","iopub.execute_input":"2022-01-18T12:18:24.708139Z","iopub.status.idle":"2022-01-18T12:18:24.729258Z","shell.execute_reply.started":"2022-01-18T12:18:24.708074Z","shell.execute_reply":"2022-01-18T12:18:24.727019Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## 檢查我們寫好的 Dataset 是否正確\n\n# 選擇第一個樣本\nsample_idx = 0\n\n# 將原始資料拿出做比較\ntext_train, label_train = dataset_train.df.iloc[sample_idx].values\n\nprint(\"Train dataset :\", label_train, text_train)\n\n# Tensor id \ntokens_tensor_train,  label_tensor_train =  dataset_train[sample_idx]\n\nprint(\"Train dataset :\", label_tensor_train,\"\\n\", tokens_tensor_train) #將文字轉成token embedding\n\n# 將 id 轉回去文字看看\nprint(\"Train dataset :\\n\",tokenizer.convert_ids_to_tokens(tokens_tensor_train.tolist()))  ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:26.344615Z","iopub.execute_input":"2022-01-18T12:18:26.344912Z","iopub.status.idle":"2022-01-18T12:18:26.369390Z","shell.execute_reply.started":"2022-01-18T12:18:26.344867Z","shell.execute_reply":"2022-01-18T12:18:26.368145Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n# 以下函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 2 tensors：\n# - tokens_tensor\n# - label_tensor\n# 它會對第一個 tensors 做 zero padding，並產生前面說明過的 masks_tensors\n\ndef create_mini_batch(samples):\n    tokens_tensors = [s[0] for s in samples]\n    \n    # 測試集有 labels\n    if samples[0][1] is not None:\n        label_ids = torch.stack([s[1] for s in samples])\n    else:\n        label_ids = None\n    \n    # zero pad 到同一序列長度\n    tokens_tensors = pad_sequence(tokens_tensors, \n                                  batch_first=True)\n  \n    \n    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n    masks_tensors = torch.zeros(tokens_tensors.shape, \n                                dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(\n        tokens_tensors != 0, 1)\n    \n    return tokens_tensors,  masks_tensors, label_ids","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:31.423800Z","iopub.execute_input":"2022-01-18T12:18:31.424472Z","iopub.status.idle":"2022-01-18T12:18:31.432414Z","shell.execute_reply.started":"2022-01-18T12:18:31.424439Z","shell.execute_reply":"2022-01-18T12:18:31.431377Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\ntrain_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\nval_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:33.824342Z","iopub.execute_input":"2022-01-18T12:18:33.824631Z","iopub.status.idle":"2022-01-18T12:18:33.831097Z","shell.execute_reply.started":"2022-01-18T12:18:33.824604Z","shell.execute_reply":"2022-01-18T12:18:33.829810Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data = next(iter(train_loader))\n\ntokens_tensors, masks_tensors, label_ids = data\n\nprint(f\"\"\"\ntokens_tensors.shape   = {tokens_tensors.shape} \n{tokens_tensors}\n------------------------\nmasks_tensors.shape    = {masks_tensors.shape}\n{masks_tensors}\n------------------------\nlabel_ids.shape        = {label_ids.shape}\n{label_ids}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:35.553075Z","iopub.execute_input":"2022-01-18T12:18:35.553654Z","iopub.status.idle":"2022-01-18T12:18:35.658465Z","shell.execute_reply.started":"2022-01-18T12:18:35.553621Z","shell.execute_reply":"2022-01-18T12:18:35.657612Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nPRETRAINED_MODEL_NAME = \"bert-base-multilingual-uncased\"\nNUM_LABELS = 2\n\nmodel = BertForSequenceClassification.from_pretrained(\n    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n\nclear_output()\n\nfor name, module in model.named_children():\n    if name == \"bert\":\n        for n, _ in module.named_children():\n            print(f\"{name}:{n}\")\n    else:\n        print(\"{:15} {}\".format(name, module))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:39.767741Z","iopub.execute_input":"2022-01-18T12:18:39.768359Z","iopub.status.idle":"2022-01-18T12:18:43.262205Z","shell.execute_reply.started":"2022-01-18T12:18:39.768325Z","shell.execute_reply":"2022-01-18T12:18:43.261006Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_predictions(model, dataloader, compute_acc=False):\n    predictions = None\n    correct = 0\n    total = 0\n      \n    with torch.no_grad():\n        # 遍巡整個資料集\n        for data in dataloader:\n            # 將所有 tensors 移到 GPU 上\n            if next(model.parameters()).is_cuda:\n                data = [t.to(\"cuda:0\") for t in data if t is not None]\n            \n            \n            tokens_tensors, masks_tensors = data[:2]\n            outputs = model(input_ids=tokens_tensors[:,0:512],  \n                            attention_mask=masks_tensors[:,0:512])\n            \n            logits = outputs[0]\n            _, pred = torch.max(logits.data, 1)\n            \n            # 用來計算訓練集的分類準確率\n            if compute_acc:\n                labels = data[2]\n                total += labels.size(0)\n                correct += (pred == labels).sum().item()\n                \n            # 將當前 batch 記錄下來\n            if predictions is None:\n                predictions = pred\n            else:\n                predictions = torch.cat((predictions, pred))\n    \n    if compute_acc:\n        acc = correct / total\n        return predictions, acc\n    return predictions\n    \n# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\nmodel = model.to(device)\n#prediction_tensor, acc = get_predictions(model, train_loader, compute_acc=True)\n#print(\"classification acc:\", acc)\n#print(prediction_tensor)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:45.164405Z","iopub.execute_input":"2022-01-18T12:18:45.165164Z","iopub.status.idle":"2022-01-18T12:18:50.752564Z","shell.execute_reply.started":"2022-01-18T12:18:45.165130Z","shell.execute_reply":"2022-01-18T12:18:50.751508Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Define optimizer function                              \noptimizer =  torch.optim.Adam(model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:51.796305Z","iopub.execute_input":"2022-01-18T12:18:51.796595Z","iopub.status.idle":"2022-01-18T12:18:51.803500Z","shell.execute_reply.started":"2022-01-18T12:18:51.796550Z","shell.execute_reply":"2022-01-18T12:18:51.801775Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train(input_data, model, optimizer):\n   \n    model.train()\n    running_loss = 0\n    for data in input_data:\n        tokens_tensors, masks_tensors, labels = [t.to(device) for t in data]\n      # 將參數梯度歸零\n        optimizer.zero_grad()\n      # forward pass\n        outputs = model(input_ids=tokens_tensors[:,0:512], \n                        attention_mask=masks_tensors[:,0:512], \n                        labels=labels)\n        loss = outputs[0]\n        loss.backward() #進行反向傳播\n        optimizer.step() #藉由反向傳播的結果計算梯度\n        running_loss += loss.item()\n    # Compute this epoch accuracy and loss\n    _, acc = get_predictions(model, input_data, compute_acc=True)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:53.636613Z","iopub.execute_input":"2022-01-18T12:18:53.636920Z","iopub.status.idle":"2022-01-18T12:18:53.645623Z","shell.execute_reply.started":"2022-01-18T12:18:53.636877Z","shell.execute_reply":"2022-01-18T12:18:53.644044Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def val(input_data, model):\n    model.eval()\n    _, acc = get_predictions(model, input_data, compute_acc=True)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:55.756327Z","iopub.execute_input":"2022-01-18T12:18:55.756598Z","iopub.status.idle":"2022-01-18T12:18:55.762453Z","shell.execute_reply.started":"2022-01-18T12:18:55.756567Z","shell.execute_reply":"2022-01-18T12:18:55.760992Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# You can adjust those hyper parameters to loop for max_epochs times           #\n################################################################################\nmax_epochs = 20\nlog_interval = 1 # print acc and loss in per log_interval time\n################################################################################\n#                               End of your code                               #\n################################################################################\ntrain_acc_list = []\nval_acc_list = []\n\n\nfor epoch in range(1, max_epochs + 1):\n    print('=' * 20, 'Epoch', epoch, '=' * 20)\n    train_acc = train(train_loader, model,optimizer)\n    val_acc = val(val_loader, model)\n\n    train_acc_list.append(train_acc)\n    val_acc_list.append(val_acc)\n    if epoch % log_interval == 0:\n        print('Train Acc: {:.6f} Val Acc: {:.6f}'.format(train_acc, val_acc))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T12:18:57.679112Z","iopub.execute_input":"2022-01-18T12:18:57.679717Z","iopub.status.idle":"2022-01-18T12:21:04.530677Z","shell.execute_reply.started":"2022-01-18T12:18:57.679682Z","shell.execute_reply":"2022-01-18T12:21:04.526402Z"},"trusted":true},"execution_count":21,"outputs":[]}]}