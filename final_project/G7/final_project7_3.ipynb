{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 二 醫學 NER model (BiLSTM-CRF)","metadata":{}},{"cell_type":"markdown","source":"我們本次的報告並沒有將醫電提供的資料進行重新建模，只是將目前現有的 NER 模型套用其中，最大的因素在於我們沒有太多時間將醫電提供的資料拆分成訓練 NER model 所需要的資料型態，在資料預處理上可說是曠日費時，但我們在讀過該篇方法後認為這或許是醫電公司未來可以考慮的方向，因此我們還是將連結與參考內容附上。","metadata":{}},{"cell_type":"code","source":"!pip install seqeval==0.0.5\n!pip install keras==2.2.4 tensorflow==1.14.0 \n!pip install h5py==2.10.0\n!pip install opencc-python-reimplemented\n!pip install git+https://www.github.com/keras-team/keras-contrib.git","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:21:38.763398Z","iopub.execute_input":"2022-01-18T09:21:38.763842Z","iopub.status.idle":"2022-01-18T09:23:02.103679Z","shell.execute_reply.started":"2022-01-18T09:21:38.763725Z","shell.execute_reply":"2022-01-18T09:23:02.102809Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model,Sequential\nfrom keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\nfrom keras_contrib.layers.crf import CRF\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport pandas as pd\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:28:29.142455Z","iopub.execute_input":"2022-01-18T09:28:29.142722Z","iopub.status.idle":"2022-01-18T09:28:29.149469Z","shell.execute_reply.started":"2022-01-18T09:28:29.142690Z","shell.execute_reply":"2022-01-18T09:28:29.148726Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!python --version\n!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:28:30.803564Z","iopub.execute_input":"2022-01-18T09:28:30.804093Z","iopub.status.idle":"2022-01-18T09:28:32.140227Z","shell.execute_reply.started":"2022-01-18T09:28:30.804052Z","shell.execute_reply":"2022-01-18T09:28:32.139336Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:28:33.922501Z","iopub.execute_input":"2022-01-18T09:28:33.922810Z","iopub.status.idle":"2022-01-18T09:28:33.927546Z","shell.execute_reply.started":"2022-01-18T09:28:33.922762Z","shell.execute_reply":"2022-01-18T09:28:33.926895Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 建立 pretrain model ","metadata":{}},{"cell_type":"markdown","source":"本模型將醫學的 NER 分成五大類，分別是 TREATMENT (治療), BODY(身體部位) , SIGNS(症狀), CHECK(診斷), DISEASE (疾病)，並且將非屬於上述五類的文辭標記為 O ，另外本次的 pretrain model 作者一共使用了 600 筆的逐字稿進行訓練，算是滿小的資料集。","metadata":{}},{"cell_type":"code","source":"class LSTMNER:\n    def __init__(self):\n        self.train_path = '../input/medical-record-nlp-for-ner-task/train.txt'\n        self.vocab_path = '../input/medical-record-nlp-for-ner-task/model/model/vocab.txt'\n        self.embedding_file = '../input/medical-record-nlp-for-ner-task/model/model/token_vec_300.bin'\n        self.model_path = '../input/medical-record-nlp-for-ner-task/model/model/tokenvec_bilstm2_crf_model_20.h5'\n        self.word_dict = self.load_worddict()\n        self.class_dict ={\n                         'O':0,\n                         'TREATMENT-I': 1,\n                         'TREATMENT-B': 2,\n                         'BODY-B': 3,\n                         'BODY-I': 4,\n                         'SIGNS-I': 5,\n                         'SIGNS-B': 6,\n                         'CHECK-B': 7,\n                         'CHECK-I': 8,\n                         'DISEASE-I': 9,\n                         'DISEASE-B': 10\n                        }\n        self.label_dict = {j:i for i,j in self.class_dict.items()}\n        self.EMBEDDING_DIM = 300\n        self.EPOCHS = 10\n        self.BATCH_SIZE = 128\n        self.NUM_CLASSES = len(self.class_dict)\n        self.VOCAB_SIZE = len(self.word_dict)\n        self.TIME_STAMPS = 150\n        self.embedding_matrix = self.build_embedding_matrix()\n        self.model = self.tokenvec_bilstm2_crf_model()\n        self.model.load_weights(self.model_path)\n\n   \n    def load_worddict(self):\n        vocabs = [line.strip() for line in open(self.vocab_path)]\n        word_dict = {wd: index for index, wd in enumerate(vocabs)}\n        return word_dict\n\n  \n    def build_input(self, text):\n        x = []\n        for char in text:\n            if char not in self.word_dict:\n                char = 'UNK'\n            x.append(self.word_dict.get(char))\n        x = pad_sequences([x], self.TIME_STAMPS)\n        return x\n\n    def predict(self, text, prin = True):\n        str = self.build_input(text)\n        raw = self.model.predict(str)[0][-self.TIME_STAMPS:]\n        result = [np.argmax(row) for row in raw]\n        chars = [i for i in text]\n        tags = [self.label_dict[i] for i in result][len(result)-len(text):]\n        res = list(zip(chars, tags))\n        if prin:\n            print(res)\n        return res\n\n    \n    def load_pretrained_embedding(self):\n        embeddings_dict = {}\n        with open(self.embedding_file, 'r') as f:\n            for line in f:\n                values = line.strip().split(' ')\n                if len(values) < 300:\n                    continue\n                word = values[0]\n                coefs = np.asarray(values[1:], dtype='float32')\n                embeddings_dict[word] = coefs\n        print('Found %s word vectors.' % len(embeddings_dict))\n        return embeddings_dict\n\n    \n    def build_embedding_matrix(self):\n        embedding_dict = self.load_pretrained_embedding()\n        embedding_matrix = np.zeros((self.VOCAB_SIZE + 1, self.EMBEDDING_DIM))\n        for word, i in self.word_dict.items():\n            embedding_vector = embedding_dict.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n\n        return embedding_matrix\n\n    \n    def tokenvec_bilstm2_crf_model(self):\n        model = Sequential()\n        embedding_layer = Embedding(self.VOCAB_SIZE + 1,\n                                    self.EMBEDDING_DIM,\n                                    weights=[self.embedding_matrix],\n                                    input_length=self.TIME_STAMPS,\n                                    trainable=False,\n                                    mask_zero=True)\n        model.add(embedding_layer)\n        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n        model.add(Dropout(0.5))\n        model.add(Bidirectional(LSTM(64, return_sequences=True)))\n        model.add(Dropout(0.5))\n        model.add(TimeDistributed(Dense(self.NUM_CLASSES)))\n        crf_layer = CRF(self.NUM_CLASSES, sparse_target=True)\n        model.add(crf_layer)\n        model.compile('adam', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])\n        model.summary()\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:29:14.351253Z","iopub.execute_input":"2022-01-18T09:29:14.351510Z","iopub.status.idle":"2022-01-18T09:29:14.374292Z","shell.execute_reply.started":"2022-01-18T09:29:14.351480Z","shell.execute_reply":"2022-01-18T09:29:14.373184Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ner = LSTMNER()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:29:32.395735Z","iopub.execute_input":"2022-01-18T09:29:32.396422Z","iopub.status.idle":"2022-01-18T09:29:37.219605Z","shell.execute_reply.started":"2022-01-18T09:29:32.396384Z","shell.execute_reply":"2022-01-18T09:29:37.218880Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 測試 pretrain model","metadata":{}},{"cell_type":"code","source":"s = input('enter an sent:').strip()\nner.predict(s)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:29:54.580820Z","iopub.execute_input":"2022-01-18T09:29:54.581504Z","iopub.status.idle":"2022-01-18T09:30:12.228065Z","shell.execute_reply.started":"2022-01-18T09:29:54.581466Z","shell.execute_reply":"2022-01-18T09:30:12.227333Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"我們發現該模型將頭痛視為是兩個不同的 NER，分別是診斷與症狀，但出現該結果最大的原因可能在於原作者訓練模型時採用的是簡體中文，但我們輸入的是繁體中文，因此我們將繁體改成簡體後再試一次。 ","metadata":{}},{"cell_type":"code","source":"from opencc import OpenCC\ncc = OpenCC('t2s')\ns = input('enter an sent:').strip()\nner.predict(cc.convert(s))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:34:57.912445Z","iopub.execute_input":"2022-01-18T09:34:57.912707Z","iopub.status.idle":"2022-01-18T09:35:00.935128Z","shell.execute_reply.started":"2022-01-18T09:34:57.912679Z","shell.execute_reply":"2022-01-18T09:35:00.934292Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"這次的結果比較符合我們的預期，因為頭痛應當是一個字詞代表著症狀。","metadata":{}},{"cell_type":"markdown","source":"## 1.3 將醫電的資料帶入模型","metadata":{}},{"cell_type":"code","source":"metadata = pd.read_csv('../input/medical-record-nlp-for-ner-task/MedData.csv',encoding = 'utf-8')\nmetadata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:56:48.101472Z","iopub.execute_input":"2022-01-18T09:56:48.102037Z","iopub.status.idle":"2022-01-18T09:56:48.122474Z","shell.execute_reply.started":"2022-01-18T09:56:48.101998Z","shell.execute_reply":"2022-01-18T09:56:48.121736Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"我們一共擷取了 96 筆資料，並且每筆資料拜代表著一筆護理師的逐字稿與交班表的內容，其中 raw data 是逐字稿的全文，而 process 是交班表中的入院經過，之所以特別挑入院經過是因為我們有朋友是護理人員，她說入院經過跟生命徵象是交班表中比較重要的內容，但是滿多的逐字稿都沒有詳細的生命徵象，因此我們將入院經過抓出，希望透過針對 raw data 進行 NER 將 NER 為 sign 的內容抓出，而 sign 的內容基本上就是該病患的入院經過，因此我們特別只抓入院經過是希望能跟經過 NER model 後的結果進行比對。 ","metadata":{}},{"cell_type":"markdown","source":"分析的步驟其實很簡單如下: \n1. 將中文以外的部分去除 (但這個部分其實不是很好，因為很多護理人員都會中英文混雜)\n2. 將繁體中文轉成檢體後丟入 pretrain 的 ner model \n3. 抓出有識別為 sign or disease 的部份","metadata":{}},{"cell_type":"code","source":"metadata['raw_data'] = metadata.raw_data.apply(lambda x: re.sub('[^\\u4e00-\\u9fa5]+', '', x))\nmetadata['medner'] = metadata.raw_data.apply(lambda x: ner.predict(cc.convert(x),prin=False))\nmetadata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:56:50.379118Z","iopub.execute_input":"2022-01-18T09:56:50.379573Z","iopub.status.idle":"2022-01-18T09:56:56.261482Z","shell.execute_reply.started":"2022-01-18T09:56:50.379533Z","shell.execute_reply":"2022-01-18T09:56:56.260720Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"metadata['medsigns'] = metadata.medner.apply(lambda x: [t for t in x if bool(re.search('SIGN|DISEASE',t[1]))] )\nmetadata2 = metadata[[bool(i) for i in metadata['medsigns']]]\nmetadata2","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:56:59.863170Z","iopub.execute_input":"2022-01-18T09:56:59.863446Z","iopub.status.idle":"2022-01-18T09:57:00.475249Z","shell.execute_reply.started":"2022-01-18T09:56:59.863414Z","shell.execute_reply":"2022-01-18T09:57:00.474556Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"我們比對 medsign 跟 process 兩個欄位的結果發現模型分類的效果其實很差，幾乎都沒有把 sign 給抓出來，我們認為可能有以下問題: \n1. 原模型在訓練的時後所採用的資料筆數過少\n2. 原模型在訓練的時候所採用的資料跟我們拿來預測的資料結構差異過大\n3. 原模型的訓練資料內容較精簡，但實際的預測資料反而過於冗長","metadata":{}},{"cell_type":"markdown","source":"因此我們將 process 的內容丟入模型當中看看結果如何，原因在於 process 的內容較精簡，我們猜效果可能會比將整段的 raw data 丟入還要更好。","metadata":{}},{"cell_type":"code","source":"metadata['process'] = metadata.process.apply(lambda x: re.sub('[^\\u4e00-\\u9fa5]+', '', x))\nmetadata['process_medner'] = metadata.process.apply(lambda x: ner.predict(cc.convert(x),prin=False))\nmetadata['processsign'] = metadata.process_medner.apply(lambda x: [t for t in x if bool(re.search('SIGN|DISEASE',t[1]))] )\nmetadata3 = metadata[[bool(i) for i in metadata['processsign']]]\nmetadata3[['process','processsign']]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T09:59:13.579682Z","iopub.execute_input":"2022-01-18T09:59:13.582593Z","iopub.status.idle":"2022-01-18T09:59:19.662452Z","shell.execute_reply.started":"2022-01-18T09:59:13.582544Z","shell.execute_reply":"2022-01-18T09:59:19.661762Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## **小節**\n由以上的嘗試我們得知，當我們只針對 process 進行 NER 預測時效果是還不錯的，基本上症狀跟疾病都有被分類出來，但我們將 raw data 整筆丟入時卻沒有辦法將其中的疾病或是症狀分類出來，我們認為最大的原因在於 raw data 包含太多沒有用到的資訊，以及 raw data 跟原模型在訓練時使用的資料長度相比差異太多，最後是我們認為中國在語法的使用上可能還是跟我們在文字的使用上有差異，導致我們無法直接將預訓練的模型直接套用。但該模型的生成方是我們認為是醫電公司可以參考且實際將資料進行分類、處理以及建模，應當能有不錯的成效。","metadata":{}}]}